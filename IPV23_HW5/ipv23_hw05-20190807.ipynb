{"cells":[{"cell_type":"markdown","source":["\n","# **[2023-1] Image Processing & Vision (55397)**\n","\n","* ### Hak Gu Kim\n","* ### Assistant Professor\n","* ### Graduate School of Advanced Imaging Science, Multimedia & Film (GSAIM)\n","* ### Chung-Ang University\n","* ### Webpage: www.irislab.cau.ac.kr\n"],"metadata":{"id":"17h2G6wX9AMS"}},{"cell_type":"markdown","source":["# **Homework V: Convolutional Neural Networks (CNNs)**\n","\n","* ### **Deadline:** 21 June (Wed) at 11:59pm\n","* ### **Submission:** Upload the zip file to \"과제 및 평가\" on E-class\n","  * **Upload zip file:** ipv23_hw05-student number.zip\n","    * **Python code:** ipv23_hw05-student number.ipynb\n","    * **Report:** ipv23_hw05-student number.pdf  (page limit: 4 pages)\n"],"metadata":{"id":"DsqcUtsx9Db5"}},{"cell_type":"markdown","source":["## **[Homework V-0]** Environmental Setting"],"metadata":{"id":"55KCnbql9Oma"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms"],"metadata":{"id":"lfseYMx1JKxy","executionInfo":{"status":"ok","timestamp":1687338013726,"user_tz":-540,"elapsed":4207,"user":{"displayName":"민정우","userId":"07570981936860096818"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **[Homework V-1]** Dataset"],"metadata":{"id":"O30LdlatWDqv"}},{"cell_type":"markdown","source":["## 1-1) MNIST Dataset\n","\n","The MNIST dataset consists of 70,000 28x28 handwritten digits images in 10 classess. 60,000 images for training and 10,000 images for test.\n","\n","- http://yann.lecun.com/exdb/mnist/\n","- https://pytorch.org/vision/stable/generated/torchvision.datasets.KMNIST.html#torchvision.datasets.KMNIST"],"metadata":{"id":"Eu6xu3AcCvWU"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"8j2ldCEJYk-P","executionInfo":{"status":"ok","timestamp":1687338016379,"user_tz":-540,"elapsed":645,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"07306cba-c7b7-412a-dfcd-59d9aab97306"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 286314380.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 42090234.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 143156518.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 26945585.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# MNIST Dataset\n","mnist_train = torchvision.datasets.MNIST(root='./', train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n","mnist_test  = torchvision.datasets.MNIST(root='./', train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n","mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n","\n","# Data Loader for MNIST\n","mnist_train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n","mnist_val_loader   = DataLoader(mnist_val, batch_size=128, shuffle=False)\n","mnist_test_loader  = DataLoader(mnist_test, batch_size=128, shuffle=False)"]},{"cell_type":"markdown","source":["## 1-2) CIFAR-10\n","\n","The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","- https://www.cs.toronto.edu/~kriz/cifar.html\n","- https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10"],"metadata":{"id":"ENgvafX9Cy2K"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"HGcXIcCW6M3O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687338028509,"user_tz":-540,"elapsed":8776,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"8a0e706a-d046-47dd-922f-6d81f7fab4ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:04<00:00, 34601469.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to ./\n","Files already downloaded and verified\n"]}],"source":["# Define the Transforms for Training Dataset\n","transforms_train = transforms.Compose([\n","  transforms.RandomCrop(32, padding=4),\n","  transforms.RandomHorizontalFlip(),\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Define the Transforms for Testing Dataset\n","transforms_test = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# CIFAR-10 Dataset\n","cifar_train = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transforms_train)\n","cifar_test = torchvision.datasets.CIFAR10(root='./', train=False, download=True, transform=transforms_test)\n","\n","# Data Loader for CIFAR-10\n","# cifar_train_loader = DataLoader(cifar_train, batch_size=128, shuffle=True)\n","# cifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n","cifar_train_loader = DataLoader(cifar_train, batch_size=128, shuffle=True, num_workers=2)\n","cifar_test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","source":["## **[Homework V-2]** (Practice) Implement Each Component of CNNs\n","\n","- Convolutional Layer\n","- Batch Normalization\n","- Dropout Layer"],"metadata":{"id":"5KkNTDf7C33J"}},{"cell_type":"markdown","source":["## 2-1) Convolutional Layer\n","\n","`nn.Conv2d`: Applies a 2D convolution over an input signal composed of several input planes.\n","\n","- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"],"metadata":{"id":"l8MUOO-FDRqc"}},{"cell_type":"markdown","source":["**Parameters for** `nn.Conv2d`\n","- in_channels (int) – Number of channels in the input image\n","\n","- out_channels (int) – Number of channels produced by the convolution\n","\n","- kernel_size (int or tuple) – Size of the convolution filter (kernel)\n","\n","- stride (int or tuple, optional) – Stride of the convolution (Default: `1`)\n","\n","- padding (int, tuple or str, optional) – Padding added to boundaries of the input (Default: `0`)\n","\n","- padding_mode (string, optional) – `zeros`, `reflect`, `replicate` or `circular` (Default: `zeros`)\n","\n","- dilation (int or tuple, optional) – Spacing between kernel elements (Default: `1`)"],"metadata":{"id":"Yz_lhYRNDW4c"}},{"cell_type":"markdown","source":["**Examples**\n","- With square kernels and equal stride:\n","\n","  `conv_layer = nn.Conv2d(16, 33, 3, stride=2)`\n","\n","- non-square kernels and unequal stride and with padding:\n","\n","  `conv_layer = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))`\n","\n","- non-square kernels and unequal stride and with padding and dilation:\n","\n","  `conv_layer = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))`"],"metadata":{"id":"SReKGxZADY9K"}},{"cell_type":"code","source":["# Example of convolutional layer\n","\n","# Input dimension: 1 x 3 x 32 x 32\n","# Convolutional layer: 32 5x5 filters with stride 2, padding 2\n","\n","x = torch.randn(1, 3, 32, 32) # input: x\n","\n","conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2, padding=2)\n","\n","print('Input size:\\n', x.size())\n","print()\n","print('Output size:\\n', conv_layer(x).size())"],"metadata":{"id":"SjfkJOIDDbHf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687284432970,"user_tz":-540,"elapsed":371,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"05f2853a-240d-4fc5-ae03-6c36181ffbfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input size:\n"," torch.Size([1, 3, 32, 32])\n","\n","Output size:\n"," torch.Size([1, 32, 16, 16])\n"]}]},{"cell_type":"markdown","source":["## 2-2) Batch Normalization\n","\n","`nn.BatchNorm2d`: Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\n","\n","- https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n","- S. Ioffe and C. Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, **ICML 2015** [[Link]](http://proceedings.mlr.press/v37/ioffe15.html)"],"metadata":{"id":"vnierdJ9DcUH"}},{"cell_type":"markdown","source":["**Parameters** for `nn.BatchNorm2d`\n","- num_features – $C$ from an expected input of size ($N, C, H, W$)"],"metadata":{"id":"u7a0k7xaDfn8"}},{"cell_type":"markdown","source":["**Example**\n","\n","- With learnable parameters\n","\n","  `bn = nn.BatchNorm2d(100)`\n"],"metadata":{"id":"NvJ_LX65Dhzs"}},{"cell_type":"code","source":["# Batch Normalization\n","\n","x = torch.randn(1, 3, 32, 32)\n","\n","bn = nn.BatchNorm2d(num_features=3)\n","\n","print('Input size:\\n', x.size())\n","print()\n","print('Size of feature after BN:\\n', bn(x).size()) # Please check the output size after the batch normalization whether the size of input is changed or not"],"metadata":{"id":"NGrwIrjdDoN-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687284435772,"user_tz":-540,"elapsed":374,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"3443bd07-6592-41fd-875f-d90a1f2e9e1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input size:\n"," torch.Size([1, 3, 32, 32])\n","\n","Size of feature after BN:\n"," torch.Size([1, 3, 32, 32])\n"]}]},{"cell_type":"markdown","source":["## **[Homework V-3]** (Practice) Build Simple Convolutional Neural Networks\n","\n","- `nn.Sequential`: A sequential container. Modules will be added to it in the order they are passed in the constructor.\n","- https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"],"metadata":{"id":"7483tUe4DkF9"}},{"cell_type":"markdown","source":["## 3-1) Define CNNs Architecture\n","\n","- 2 conv layers with 7x7 kernel (Convolution + Batch normalization + ReLU)\n","- 1 fc layer for 10 classes"],"metadata":{"id":"PYljB4n1EBS4"}},{"cell_type":"code","source":["# Model: Simple Convolutional Neural Networks\n","\n","class ConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        # 1 input image channel, 32 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer1 = nn.Sequential(\n","            nn.Conv2d(1, 32, 7),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","        )\n","        # 32 input image channel, 64 output channels, 7x7 square convolution, 1 stride\n","        self.conv_layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, 7),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.fc = nn.Linear(64*16*16, 10)\n","\n","    def forward(self, x):\n","        out_conv1 = self.conv_layer1(x)\n","        out_conv2 = self.conv_layer2(out_conv1)\n","        feature_1d = torch.flatten(out_conv2, 1)\n","        out = self.fc(feature_1d)\n","        return out\n"],"metadata":{"id":"svbOAiGQEiWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using GPU\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = ConvNet()\n","model = model.to(device)"],"metadata":{"id":"qAZlEsqyEqic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687328448173,"user_tz":-540,"elapsed":4666,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"58866925-6714-497a-cea9-a9a98504b8eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["## 3-2) Define Optimizer & Loss\n","- Optimization using stochastic gradient descent (SGD)\n","- Learning rate α=0.01\n","- Loss function: Cross Entropy Loss"],"metadata":{"id":"VbKT8XHjEh7K"}},{"cell_type":"code","source":["# Optimizer: Stochastic Gradient Descent Method\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"d60kmY8VE0vH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define Loss function (Cross Entropy Loss here)\n","\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"9ag_vvvVFOdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3-3) Train the Simple CNNs Model\n","- Dataset: MNIST\n","- Epochs = 10"],"metadata":{"id":"MbxJl9jjFS5w"}},{"cell_type":"code","source":["# Train the model\n","total_step = len(mnist_train_loader)\n","epochs = 10\n","for epoch in range(epochs):\n","    for i, (images, labels) in enumerate(mnist_train_loader):  # mini batch for loop\n","\n","        # Upload to gpu\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass & Optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"],"metadata":{"id":"Udiwr4k_Ffnc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687328533839,"user_tz":-540,"elapsed":80611,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"eca4b5a8-5779-43c7-85d2-73928717b4d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [100/391], Loss: 0.1928\n","Epoch [1/10], Step [200/391], Loss: 0.1565\n","Epoch [1/10], Step [300/391], Loss: 0.1431\n","Epoch [2/10], Step [100/391], Loss: 0.0854\n","Epoch [2/10], Step [200/391], Loss: 0.0815\n","Epoch [2/10], Step [300/391], Loss: 0.0557\n","Epoch [3/10], Step [100/391], Loss: 0.0310\n","Epoch [3/10], Step [200/391], Loss: 0.0226\n","Epoch [3/10], Step [300/391], Loss: 0.0202\n","Epoch [4/10], Step [100/391], Loss: 0.0771\n","Epoch [4/10], Step [200/391], Loss: 0.0344\n","Epoch [4/10], Step [300/391], Loss: 0.0741\n","Epoch [5/10], Step [100/391], Loss: 0.0273\n","Epoch [5/10], Step [200/391], Loss: 0.1113\n","Epoch [5/10], Step [300/391], Loss: 0.0229\n","Epoch [6/10], Step [100/391], Loss: 0.0142\n","Epoch [6/10], Step [200/391], Loss: 0.0167\n","Epoch [6/10], Step [300/391], Loss: 0.0434\n","Epoch [7/10], Step [100/391], Loss: 0.0269\n","Epoch [7/10], Step [200/391], Loss: 0.0109\n","Epoch [7/10], Step [300/391], Loss: 0.0387\n","Epoch [8/10], Step [100/391], Loss: 0.0105\n","Epoch [8/10], Step [200/391], Loss: 0.0117\n","Epoch [8/10], Step [300/391], Loss: 0.0198\n","Epoch [9/10], Step [100/391], Loss: 0.0219\n","Epoch [9/10], Step [200/391], Loss: 0.0287\n","Epoch [9/10], Step [300/391], Loss: 0.0093\n","Epoch [10/10], Step [100/391], Loss: 0.0130\n","Epoch [10/10], Step [200/391], Loss: 0.0270\n","Epoch [10/10], Step [300/391], Loss: 0.0276\n"]}]},{"cell_type":"markdown","source":["##3-4) Test the Trained CNNs Model"],"metadata":{"id":"4ShRnU-tFrZX"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in mnist_test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","\n","        _, predicted = torch.max(outputs.data, 1)  # classificatoin model -> get the label prediction of top 1\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of Simple CNN on MNIST test set: {} %'.format(100 * correct / total))"],"metadata":{"id":"qZs4OU6sF0U1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687328548801,"user_tz":-540,"elapsed":2363,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"54a668c7-3607-495a-c744-a78e2d26f5c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Simple CNN on MNIST test set: 99.09 %\n"]}]},{"cell_type":"markdown","source":["## **[Homework V-4]** Design Your Own Convolutional Neural Networks\n","**References**\n","\n","https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n","\n","\n","**Options**\n","- The number of convolutional layers\n","- Stride & padding & dilation\n","- Various activation functions\n","- Pooling layers (max pool, avg pool)\n","- The number of fully connected layers\n","- The dimension of hidden layers\n","- The size of kernels at each layer\n","- *etc*."],"metadata":{"id":"pEEgEYmRF4J4"}},{"cell_type":"code","source":["# Change the following CNNs architecture\n","\n","class myConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(myConvNet, self).__init__()\n","        self.conv_layer1 = nn.Sequential(\n","            nn.Conv2d(3, 64, 3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","        self.conv_layer2 = nn.Sequential(\n","            nn.Conv2d(64, 128, 3, stride = 2),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","        )\n","        self.conv_layer3 = nn.Sequential(\n","            nn.Conv2d(128, 256, 3, stride = 2),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","        )\n","        self.conv_layer4 = nn.Sequential(\n","            nn.Conv2d(256, 512, 3, stride = 2),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","        )\n","\n","        self.fc = nn.Linear(512*2*2, 10)\n","\n","    def forward(self, x):\n","        out_conv1 = self.conv_layer1(x)\n","        out_conv2 = self.conv_layer2(out_conv1)\n","        out_conv3 = self.conv_layer3(out_conv2)\n","        out_conv4 = self.conv_layer4(out_conv3)\n","        feature_1d = torch.flatten(out_conv4, 1)\n","        out = self.fc(feature_1d)\n","        return out\n"],"metadata":{"id":"caCYEDDiJV4N","executionInfo":{"status":"ok","timestamp":1687338036906,"user_tz":-540,"elapsed":311,"user":{"displayName":"민정우","userId":"07570981936860096818"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model = myConvNet()\n","model = model.to(device)"],"metadata":{"id":"vto9LzS_Jymy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687338044042,"user_tz":-540,"elapsed":5003,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"548bcca6-c782-4341-e131-ccfccb2d0ec2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["##4-1) Train Your CNNs Model\n","You can change the number of epochs, learning rate, optimizer, *etc*."],"metadata":{"id":"jHrFQ1rVKDad"}},{"cell_type":"code","source":["# Optimizer: Stochastic Gradient Descent Method\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","\n","# Define Loss function\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"Zt25_qGLJ2LV","executionInfo":{"status":"ok","timestamp":1687338045319,"user_tz":-540,"elapsed":303,"user":{"displayName":"민정우","userId":"07570981936860096818"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","total_step = len(cifar_train_loader)\n","epochs = 100\n","for epoch in range(epochs):\n","    for i, (images, labels) in enumerate(cifar_train_loader):  # mini batch for loop\n","\n","        # Upload to gpu\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass & Optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"],"metadata":{"id":"Gt-Va_0NKRhb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687340145125,"user_tz":-540,"elapsed":2098225,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"6295417d-1654-4e54-9583-d90551b85541"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Step [100/391], Loss: 2.0959\n","Epoch [1/100], Step [200/391], Loss: 2.0995\n","Epoch [1/100], Step [300/391], Loss: 1.9299\n","Epoch [2/100], Step [100/391], Loss: 1.7750\n","Epoch [2/100], Step [200/391], Loss: 1.7752\n","Epoch [2/100], Step [300/391], Loss: 1.7798\n","Epoch [3/100], Step [100/391], Loss: 1.7298\n","Epoch [3/100], Step [200/391], Loss: 1.8390\n","Epoch [3/100], Step [300/391], Loss: 1.8153\n","Epoch [4/100], Step [100/391], Loss: 1.6469\n","Epoch [4/100], Step [200/391], Loss: 1.6488\n","Epoch [4/100], Step [300/391], Loss: 1.6068\n","Epoch [5/100], Step [100/391], Loss: 1.6723\n","Epoch [5/100], Step [200/391], Loss: 1.6586\n","Epoch [5/100], Step [300/391], Loss: 1.6968\n","Epoch [6/100], Step [100/391], Loss: 1.5633\n","Epoch [6/100], Step [200/391], Loss: 1.6100\n","Epoch [6/100], Step [300/391], Loss: 1.5539\n","Epoch [7/100], Step [100/391], Loss: 1.6915\n","Epoch [7/100], Step [200/391], Loss: 1.3735\n","Epoch [7/100], Step [300/391], Loss: 1.4686\n","Epoch [8/100], Step [100/391], Loss: 1.6153\n","Epoch [8/100], Step [200/391], Loss: 1.4878\n","Epoch [8/100], Step [300/391], Loss: 1.5714\n","Epoch [9/100], Step [100/391], Loss: 1.5949\n","Epoch [9/100], Step [200/391], Loss: 1.5912\n","Epoch [9/100], Step [300/391], Loss: 1.5259\n","Epoch [10/100], Step [100/391], Loss: 1.4788\n","Epoch [10/100], Step [200/391], Loss: 1.3704\n","Epoch [10/100], Step [300/391], Loss: 1.5159\n","Epoch [11/100], Step [100/391], Loss: 1.4696\n","Epoch [11/100], Step [200/391], Loss: 1.4415\n","Epoch [11/100], Step [300/391], Loss: 1.5162\n","Epoch [12/100], Step [100/391], Loss: 1.3948\n","Epoch [12/100], Step [200/391], Loss: 1.4237\n","Epoch [12/100], Step [300/391], Loss: 1.5494\n","Epoch [13/100], Step [100/391], Loss: 1.3321\n","Epoch [13/100], Step [200/391], Loss: 1.4878\n","Epoch [13/100], Step [300/391], Loss: 1.5515\n","Epoch [14/100], Step [100/391], Loss: 1.3901\n","Epoch [14/100], Step [200/391], Loss: 1.5094\n","Epoch [14/100], Step [300/391], Loss: 1.3094\n","Epoch [15/100], Step [100/391], Loss: 1.3251\n","Epoch [15/100], Step [200/391], Loss: 1.4189\n","Epoch [15/100], Step [300/391], Loss: 1.4606\n","Epoch [16/100], Step [100/391], Loss: 1.3486\n","Epoch [16/100], Step [200/391], Loss: 1.3359\n","Epoch [16/100], Step [300/391], Loss: 1.3050\n","Epoch [17/100], Step [100/391], Loss: 1.4425\n","Epoch [17/100], Step [200/391], Loss: 1.2441\n","Epoch [17/100], Step [300/391], Loss: 1.3930\n","Epoch [18/100], Step [100/391], Loss: 1.4293\n","Epoch [18/100], Step [200/391], Loss: 1.2652\n","Epoch [18/100], Step [300/391], Loss: 1.4202\n","Epoch [19/100], Step [100/391], Loss: 1.1516\n","Epoch [19/100], Step [200/391], Loss: 1.2282\n","Epoch [19/100], Step [300/391], Loss: 1.3382\n","Epoch [20/100], Step [100/391], Loss: 1.2668\n","Epoch [20/100], Step [200/391], Loss: 1.2870\n","Epoch [20/100], Step [300/391], Loss: 1.1879\n","Epoch [21/100], Step [100/391], Loss: 1.3495\n","Epoch [21/100], Step [200/391], Loss: 1.1951\n","Epoch [21/100], Step [300/391], Loss: 1.2150\n","Epoch [22/100], Step [100/391], Loss: 1.1773\n","Epoch [22/100], Step [200/391], Loss: 1.2903\n","Epoch [22/100], Step [300/391], Loss: 1.2802\n","Epoch [23/100], Step [100/391], Loss: 1.3550\n","Epoch [23/100], Step [200/391], Loss: 1.2715\n","Epoch [23/100], Step [300/391], Loss: 1.2704\n","Epoch [24/100], Step [100/391], Loss: 1.1462\n","Epoch [24/100], Step [200/391], Loss: 1.2141\n","Epoch [24/100], Step [300/391], Loss: 1.2529\n","Epoch [25/100], Step [100/391], Loss: 1.2095\n","Epoch [25/100], Step [200/391], Loss: 1.2456\n","Epoch [25/100], Step [300/391], Loss: 1.2709\n","Epoch [26/100], Step [100/391], Loss: 1.1577\n","Epoch [26/100], Step [200/391], Loss: 1.3035\n","Epoch [26/100], Step [300/391], Loss: 1.3214\n","Epoch [27/100], Step [100/391], Loss: 1.1899\n","Epoch [27/100], Step [200/391], Loss: 1.3135\n","Epoch [27/100], Step [300/391], Loss: 1.2168\n","Epoch [28/100], Step [100/391], Loss: 1.3757\n","Epoch [28/100], Step [200/391], Loss: 1.0951\n","Epoch [28/100], Step [300/391], Loss: 1.3106\n","Epoch [29/100], Step [100/391], Loss: 1.0242\n","Epoch [29/100], Step [200/391], Loss: 1.2066\n","Epoch [29/100], Step [300/391], Loss: 1.2404\n","Epoch [30/100], Step [100/391], Loss: 1.2102\n","Epoch [30/100], Step [200/391], Loss: 1.0851\n","Epoch [30/100], Step [300/391], Loss: 1.0992\n","Epoch [31/100], Step [100/391], Loss: 0.9899\n","Epoch [31/100], Step [200/391], Loss: 1.1295\n","Epoch [31/100], Step [300/391], Loss: 1.2139\n","Epoch [32/100], Step [100/391], Loss: 1.2284\n","Epoch [32/100], Step [200/391], Loss: 1.2480\n","Epoch [32/100], Step [300/391], Loss: 1.1406\n","Epoch [33/100], Step [100/391], Loss: 1.1010\n","Epoch [33/100], Step [200/391], Loss: 1.1379\n","Epoch [33/100], Step [300/391], Loss: 1.0854\n","Epoch [34/100], Step [100/391], Loss: 0.9851\n","Epoch [34/100], Step [200/391], Loss: 1.1559\n","Epoch [34/100], Step [300/391], Loss: 1.1594\n","Epoch [35/100], Step [100/391], Loss: 0.9954\n","Epoch [35/100], Step [200/391], Loss: 0.9950\n","Epoch [35/100], Step [300/391], Loss: 1.2118\n","Epoch [36/100], Step [100/391], Loss: 1.2512\n","Epoch [36/100], Step [200/391], Loss: 1.1027\n","Epoch [36/100], Step [300/391], Loss: 1.3699\n","Epoch [37/100], Step [100/391], Loss: 1.0593\n","Epoch [37/100], Step [200/391], Loss: 1.0548\n","Epoch [37/100], Step [300/391], Loss: 1.1017\n","Epoch [38/100], Step [100/391], Loss: 1.0534\n","Epoch [38/100], Step [200/391], Loss: 1.1078\n","Epoch [38/100], Step [300/391], Loss: 1.1718\n","Epoch [39/100], Step [100/391], Loss: 1.1355\n","Epoch [39/100], Step [200/391], Loss: 1.0192\n","Epoch [39/100], Step [300/391], Loss: 1.1233\n","Epoch [40/100], Step [100/391], Loss: 1.2599\n","Epoch [40/100], Step [200/391], Loss: 0.9863\n","Epoch [40/100], Step [300/391], Loss: 1.0527\n","Epoch [41/100], Step [100/391], Loss: 1.1547\n","Epoch [41/100], Step [200/391], Loss: 1.1522\n","Epoch [41/100], Step [300/391], Loss: 1.1755\n","Epoch [42/100], Step [100/391], Loss: 0.9730\n","Epoch [42/100], Step [200/391], Loss: 1.1130\n","Epoch [42/100], Step [300/391], Loss: 1.0333\n","Epoch [43/100], Step [100/391], Loss: 0.9892\n","Epoch [43/100], Step [200/391], Loss: 1.1095\n","Epoch [43/100], Step [300/391], Loss: 1.0013\n","Epoch [44/100], Step [100/391], Loss: 1.1157\n","Epoch [44/100], Step [200/391], Loss: 0.9852\n","Epoch [44/100], Step [300/391], Loss: 1.0017\n","Epoch [45/100], Step [100/391], Loss: 1.1849\n","Epoch [45/100], Step [200/391], Loss: 0.9553\n","Epoch [45/100], Step [300/391], Loss: 1.0438\n","Epoch [46/100], Step [100/391], Loss: 0.9936\n","Epoch [46/100], Step [200/391], Loss: 1.1487\n","Epoch [46/100], Step [300/391], Loss: 0.8840\n","Epoch [47/100], Step [100/391], Loss: 1.0433\n","Epoch [47/100], Step [200/391], Loss: 1.0507\n","Epoch [47/100], Step [300/391], Loss: 1.1742\n","Epoch [48/100], Step [100/391], Loss: 1.0923\n","Epoch [48/100], Step [200/391], Loss: 1.0936\n","Epoch [48/100], Step [300/391], Loss: 0.9887\n","Epoch [49/100], Step [100/391], Loss: 1.0812\n","Epoch [49/100], Step [200/391], Loss: 0.9967\n","Epoch [49/100], Step [300/391], Loss: 1.0552\n","Epoch [50/100], Step [100/391], Loss: 1.0712\n","Epoch [50/100], Step [200/391], Loss: 0.9198\n","Epoch [50/100], Step [300/391], Loss: 0.9194\n","Epoch [51/100], Step [100/391], Loss: 0.8970\n","Epoch [51/100], Step [200/391], Loss: 0.9493\n","Epoch [51/100], Step [300/391], Loss: 0.8923\n","Epoch [52/100], Step [100/391], Loss: 0.9825\n","Epoch [52/100], Step [200/391], Loss: 0.8808\n","Epoch [52/100], Step [300/391], Loss: 0.9618\n","Epoch [53/100], Step [100/391], Loss: 1.2431\n","Epoch [53/100], Step [200/391], Loss: 1.0975\n","Epoch [53/100], Step [300/391], Loss: 1.0516\n","Epoch [54/100], Step [100/391], Loss: 0.8771\n","Epoch [54/100], Step [200/391], Loss: 1.0032\n","Epoch [54/100], Step [300/391], Loss: 1.0177\n","Epoch [55/100], Step [100/391], Loss: 0.8701\n","Epoch [55/100], Step [200/391], Loss: 0.9250\n","Epoch [55/100], Step [300/391], Loss: 1.0425\n","Epoch [56/100], Step [100/391], Loss: 1.0981\n","Epoch [56/100], Step [200/391], Loss: 0.8282\n","Epoch [56/100], Step [300/391], Loss: 1.3271\n","Epoch [57/100], Step [100/391], Loss: 0.8862\n","Epoch [57/100], Step [200/391], Loss: 1.1020\n","Epoch [57/100], Step [300/391], Loss: 1.0475\n","Epoch [58/100], Step [100/391], Loss: 1.0074\n","Epoch [58/100], Step [200/391], Loss: 1.0903\n","Epoch [58/100], Step [300/391], Loss: 0.9031\n","Epoch [59/100], Step [100/391], Loss: 1.0588\n","Epoch [59/100], Step [200/391], Loss: 0.8545\n","Epoch [59/100], Step [300/391], Loss: 0.9346\n","Epoch [60/100], Step [100/391], Loss: 0.8459\n","Epoch [60/100], Step [200/391], Loss: 0.9643\n","Epoch [60/100], Step [300/391], Loss: 0.9494\n","Epoch [61/100], Step [100/391], Loss: 1.0467\n","Epoch [61/100], Step [200/391], Loss: 0.9563\n","Epoch [61/100], Step [300/391], Loss: 0.8741\n","Epoch [62/100], Step [100/391], Loss: 0.9600\n","Epoch [62/100], Step [200/391], Loss: 0.8408\n","Epoch [62/100], Step [300/391], Loss: 0.9825\n","Epoch [63/100], Step [100/391], Loss: 0.8613\n","Epoch [63/100], Step [200/391], Loss: 1.0607\n","Epoch [63/100], Step [300/391], Loss: 0.8013\n","Epoch [64/100], Step [100/391], Loss: 0.9307\n","Epoch [64/100], Step [200/391], Loss: 0.8112\n","Epoch [64/100], Step [300/391], Loss: 0.7986\n","Epoch [65/100], Step [100/391], Loss: 0.8894\n","Epoch [65/100], Step [200/391], Loss: 0.9182\n","Epoch [65/100], Step [300/391], Loss: 0.9049\n","Epoch [66/100], Step [100/391], Loss: 1.0782\n","Epoch [66/100], Step [200/391], Loss: 0.8556\n","Epoch [66/100], Step [300/391], Loss: 0.9122\n","Epoch [67/100], Step [100/391], Loss: 0.8460\n","Epoch [67/100], Step [200/391], Loss: 0.8338\n","Epoch [67/100], Step [300/391], Loss: 0.9370\n","Epoch [68/100], Step [100/391], Loss: 1.0135\n","Epoch [68/100], Step [200/391], Loss: 0.8580\n","Epoch [68/100], Step [300/391], Loss: 0.9011\n","Epoch [69/100], Step [100/391], Loss: 0.8405\n","Epoch [69/100], Step [200/391], Loss: 0.8478\n","Epoch [69/100], Step [300/391], Loss: 1.0933\n","Epoch [70/100], Step [100/391], Loss: 0.8445\n","Epoch [70/100], Step [200/391], Loss: 0.8666\n","Epoch [70/100], Step [300/391], Loss: 0.9021\n","Epoch [71/100], Step [100/391], Loss: 0.9381\n","Epoch [71/100], Step [200/391], Loss: 0.8745\n","Epoch [71/100], Step [300/391], Loss: 0.9692\n","Epoch [72/100], Step [100/391], Loss: 0.8595\n","Epoch [72/100], Step [200/391], Loss: 1.0099\n","Epoch [72/100], Step [300/391], Loss: 1.0001\n","Epoch [73/100], Step [100/391], Loss: 0.9103\n","Epoch [73/100], Step [200/391], Loss: 0.9087\n","Epoch [73/100], Step [300/391], Loss: 0.8267\n","Epoch [74/100], Step [100/391], Loss: 0.8954\n","Epoch [74/100], Step [200/391], Loss: 0.8123\n","Epoch [74/100], Step [300/391], Loss: 0.9539\n","Epoch [75/100], Step [100/391], Loss: 0.6476\n","Epoch [75/100], Step [200/391], Loss: 0.8503\n","Epoch [75/100], Step [300/391], Loss: 0.9227\n","Epoch [76/100], Step [100/391], Loss: 0.9442\n","Epoch [76/100], Step [200/391], Loss: 0.8535\n","Epoch [76/100], Step [300/391], Loss: 0.8672\n","Epoch [77/100], Step [100/391], Loss: 0.8432\n","Epoch [77/100], Step [200/391], Loss: 0.9090\n","Epoch [77/100], Step [300/391], Loss: 0.7562\n","Epoch [78/100], Step [100/391], Loss: 0.8326\n","Epoch [78/100], Step [200/391], Loss: 0.9405\n","Epoch [78/100], Step [300/391], Loss: 0.9116\n","Epoch [79/100], Step [100/391], Loss: 0.9344\n","Epoch [79/100], Step [200/391], Loss: 0.8111\n","Epoch [79/100], Step [300/391], Loss: 0.7137\n","Epoch [80/100], Step [100/391], Loss: 0.8265\n","Epoch [80/100], Step [200/391], Loss: 1.1079\n","Epoch [80/100], Step [300/391], Loss: 0.8802\n","Epoch [81/100], Step [100/391], Loss: 0.9743\n","Epoch [81/100], Step [200/391], Loss: 0.8291\n","Epoch [81/100], Step [300/391], Loss: 0.8926\n","Epoch [82/100], Step [100/391], Loss: 0.8572\n","Epoch [82/100], Step [200/391], Loss: 0.8335\n","Epoch [82/100], Step [300/391], Loss: 0.8459\n","Epoch [83/100], Step [100/391], Loss: 0.9308\n","Epoch [83/100], Step [200/391], Loss: 0.8192\n","Epoch [83/100], Step [300/391], Loss: 0.8229\n","Epoch [84/100], Step [100/391], Loss: 0.8808\n","Epoch [84/100], Step [200/391], Loss: 0.7328\n","Epoch [84/100], Step [300/391], Loss: 0.6148\n","Epoch [85/100], Step [100/391], Loss: 0.7879\n","Epoch [85/100], Step [200/391], Loss: 0.8037\n","Epoch [85/100], Step [300/391], Loss: 1.0237\n","Epoch [86/100], Step [100/391], Loss: 0.7894\n","Epoch [86/100], Step [200/391], Loss: 0.8503\n","Epoch [86/100], Step [300/391], Loss: 0.9343\n","Epoch [87/100], Step [100/391], Loss: 1.0102\n","Epoch [87/100], Step [200/391], Loss: 0.9235\n","Epoch [87/100], Step [300/391], Loss: 0.8266\n","Epoch [88/100], Step [100/391], Loss: 0.9702\n","Epoch [88/100], Step [200/391], Loss: 0.6998\n","Epoch [88/100], Step [300/391], Loss: 0.7700\n","Epoch [89/100], Step [100/391], Loss: 0.9106\n","Epoch [89/100], Step [200/391], Loss: 0.7409\n","Epoch [89/100], Step [300/391], Loss: 0.7942\n","Epoch [90/100], Step [100/391], Loss: 0.7708\n","Epoch [90/100], Step [200/391], Loss: 0.8435\n","Epoch [90/100], Step [300/391], Loss: 0.7855\n","Epoch [91/100], Step [100/391], Loss: 0.6740\n","Epoch [91/100], Step [200/391], Loss: 0.6709\n","Epoch [91/100], Step [300/391], Loss: 0.7228\n","Epoch [92/100], Step [100/391], Loss: 0.7795\n","Epoch [92/100], Step [200/391], Loss: 0.8935\n","Epoch [92/100], Step [300/391], Loss: 0.8307\n","Epoch [93/100], Step [100/391], Loss: 0.7064\n","Epoch [93/100], Step [200/391], Loss: 0.7517\n","Epoch [93/100], Step [300/391], Loss: 0.6443\n","Epoch [94/100], Step [100/391], Loss: 0.7893\n","Epoch [94/100], Step [200/391], Loss: 0.8726\n","Epoch [94/100], Step [300/391], Loss: 0.7986\n","Epoch [95/100], Step [100/391], Loss: 0.8371\n","Epoch [95/100], Step [200/391], Loss: 0.7938\n","Epoch [95/100], Step [300/391], Loss: 1.0495\n","Epoch [96/100], Step [100/391], Loss: 0.9852\n","Epoch [96/100], Step [200/391], Loss: 0.7638\n","Epoch [96/100], Step [300/391], Loss: 0.9851\n","Epoch [97/100], Step [100/391], Loss: 0.7904\n","Epoch [97/100], Step [200/391], Loss: 0.6291\n","Epoch [97/100], Step [300/391], Loss: 0.8150\n","Epoch [98/100], Step [100/391], Loss: 0.7552\n","Epoch [98/100], Step [200/391], Loss: 0.8887\n","Epoch [98/100], Step [300/391], Loss: 0.7892\n","Epoch [99/100], Step [100/391], Loss: 0.7991\n","Epoch [99/100], Step [200/391], Loss: 0.8860\n","Epoch [99/100], Step [300/391], Loss: 0.7438\n","Epoch [100/100], Step [100/391], Loss: 0.8508\n","Epoch [100/100], Step [200/391], Loss: 0.8336\n","Epoch [100/100], Step [300/391], Loss: 0.7766\n"]}]},{"cell_type":"markdown","source":["##4-2) Test the Trained Your CNNs Model\n","Try to acheive the best performance!"],"metadata":{"id":"SdzqgVFuKmKn"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in cifar_test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(images)\n","\n","        _, predicted = torch.max(outputs.data, 1)  # classificatoin model -> get the label prediction of top 1\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of Your CNNs on CIFAR-10 test set: {} %'.format(100 * correct / total))"],"metadata":{"id":"g9cJL-QpKq4F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687340299542,"user_tz":-540,"elapsed":3815,"user":{"displayName":"민정우","userId":"07570981936860096818"}},"outputId":"1aa7036f-0b38-40ae-d056-6081eea95e62"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Your CNNs on CIFAR-10 test set: 73.72 %\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}