{"cells":[{"cell_type":"markdown","metadata":{"id":"QSKlCRkoHiL8"},"source":["\n","# **[2023-1] Image Processing \u0026 Vision (55397)**\n","\n","* ### Hak Gu Kim\n","* ### Assistant Professor\n","* ### Graduate School of Advanced Imaging Science, Multimedia \u0026 Film (GSAIM)\n","* ### Chung-Ang University\n","* ### Webpage: www.irislab.cau.ac.kr\n"]},{"cell_type":"markdown","metadata":{"id":"9F5DyIzWHvE0"},"source":["# **Homework III: Panorama Stitching**\n","\n","* ### **Deadline:** 28 April (Fri) at 11:59pm\n","* ### **Submission:** Upload the zip file to \"과제 및 평가\" on E-class\n","  * **Upload zip file:** ipv23_hw03-student number.zip\n","    * **Python code:** ipv23_hw03-student number.ipynb\n","    * **Report:** ipv23_hw03-student number.pdf  (page limit: 4 pages)\n","  "]},{"cell_type":"markdown","metadata":{"id":"cZUui5N9OzIj"},"source":["## **[Homework III-0]** Environmental Setting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7545,"status":"ok","timestamp":1681375533236,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"6P6dLTDyebqb","outputId":"aa6fb06c-1730-417d-b6d6-5b5387d100bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opencv-contrib-python==4.4.0.46\n","  Downloading opencv_contrib_python-4.4.0.46-cp39-cp39-manylinux2014_x86_64.whl (55.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.19.3 in /usr/local/lib/python3.9/dist-packages (from opencv-contrib-python==4.4.0.46) (1.22.4)\n","Installing collected packages: opencv-contrib-python\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.7.0.72\n","    Uninstalling opencv-contrib-python-4.7.0.72:\n","      Successfully uninstalled opencv-contrib-python-4.7.0.72\n","Successfully installed opencv-contrib-python-4.4.0.46\n"]}],"source":["!pip install opencv-contrib-python==4.4.0.46"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17927,"status":"ok","timestamp":1682154910732,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"9wQvfpAnhB3T","outputId":"5e5ec6d8-5395-4519-aae2-fdf2a275a440"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["# Connect to the google drive #\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":473,"status":"ok","timestamp":1682154913163,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"OZ4vG1FEJZPG"},"outputs":[],"source":["# Import the required libraries for image processing\n","\n","import sys\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","import math\n","import random\n","from tqdm.notebook import tqdm\n","plt.rcParams['figure.figsize'] = [15, 15]\n","\n","# Define the directory\n","dir = '/content/gdrive/My Drive/Colab Notebooks/IPV23_HW3/test imgs/' # File path"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682154915802,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"zfV0T9_PJmNm"},"outputs":[],"source":["# Define the functions for the load and save the input image\n","\n","def loadImg(in_fname):\n","  img = cv2.imread(dir + in_fname)\n","\n","  if img is None:\n","    print('Image load failed!')\n","    sys.exit()\n","\n","  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","  plt.title(\"Input RGB Image\")\n","  plt.show()\n","\n","  return img  \n","\n","## Save image file\n","def saveImg(out_img, out_fname):\n","  cv2.imwrite(dir + out_fname, out_img)"]},{"cell_type":"markdown","metadata":{"id":"9SJED_18PZFr"},"source":["## **[Homework III-1]** SIFT Feature Extraction and Matching for Panorama Stitching\n","* There is nothing for you to do in this field\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682154917567,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"8j2ldCEJYk-P"},"outputs":[],"source":["##############################################\n","#             Feature Extraction             #\n","##############################################\n","def SIFT(img):\n","    \"\"\"\n","    SIFT(img) -\u003e kp, des\\n\n","    Compute a SIFT operation on the image and return keypoints and descriptors.\n","\n","    input\n","        img(ndarray) : gray color image\n","    output\n","        kp(tuple) : a list of result keypoints of SIFT\n","        des(ndarray) : descriptors array of shape (number of keypoints) * 128\n","    \"\"\"\n","\n","    siftDetector = cv2.xfeatures2d.SIFT_create() # limit 1000 points\n","    # siftDetector= cv2.SIFT_create()  # depends on OpenCV version\n","\n","    kp, des = siftDetector.detectAndCompute(img, None)\n","    return kp, des  # keypoints \u0026 descriptors\n","\n","\n","##############################################\n","#               Feature Matching             #\n","##############################################\n","def siftMatch(kp1, des1, img1, kp2, des2, img2, threshold):\n","    \"\"\"\n","    siftMatch(kp1, des1, img1, kp2, des2, img2, threshold) -\u003e matches\\n\n","    Match the keypoints of two images through descriptors.\n","\n","    input\n","        kp(tuple) : keypoints of each image\n","        des(ndarray) : descriptors of each image\n","        img(ndarray) : Not used in function\n","        threshold(float) : threshold of matching\n","    output\n","        matches(ndarray) : 2d array of matched keypoint coordinates\n","        [[queryIdx.pt.x, queryIdx.pt.y, trainIdx.pt.x, trainIdx.pt.y], [...], ...]\n","    \"\"\" \n","\n","    bf = cv2.BFMatcher()\n","    matches = bf.knnMatch(des1,des2, k=2)\n","\n","    # Apply ratio test\n","    good = []\n","    for m,n in matches:\n","        if m.distance \u003c threshold*n.distance:\n","            good.append([m])\n","\n","    matches = []\n","    for pair in good:\n","        matches.append(list(kp1[pair[0].queryIdx].pt + kp2[pair[0].trainIdx].pt))\n","\n","    matches = np.array(matches)\n","    return matches\n","\n","\n","##############################################\n","#          Transformation Matrix, H          #\n","##############################################\n","def homography(pairs):\n","    \"\"\"\n","    homography(pairs) -\u003e H\\n\n","    Calculate projective transform matrix.\n","    \n","    input\n","        pairs(ndarray) : array of point pairs\n","    output\n","        H(ndarray) : 3x3 projective transformation matrix\n","    \"\"\"\n","\n","    rows = []\n","    for i in range(pairs.shape[0]):\n","        p1 = np.append(pairs[i][0:2], 1)\n","        p2 = np.append(pairs[i][2:4], 1)\n","        row1 = [0, 0, 0, p1[0], p1[1], p1[2], -p2[1]*p1[0], -p2[1]*p1[1], -p2[1]*p1[2]]\n","        row2 = [p1[0], p1[1], p1[2], 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1], -p2[0]*p1[2]]\n","        rows.append(row1)\n","        rows.append(row2)\n","\n","    rows = np.array(rows)\n","    U, s, V = np.linalg.svd(rows) # singular value decomposition\n","    H = V[-1].reshape(3, 3)\n","    H = H/H[2, 2] # standardize to let w*H[2,2] = 1\n","    return H\n","\n","\n","##############################################\n","#         Panoramic Image Generation         #\n","##############################################\n","def stitchImg(left, right, H):\n","    \"\"\"\n","    stitchImg(left, right, H) -\u003e stitch_image\\n\n","    Stitch the two images through the computed projective transformation matrix.\n","\n","    input\n","        left(ndarray) : left image to stitch\n","        right(ndarray) : right image to stitch\n","        H(ndarray) : 3x3 projective transformation matrix\n","    output\n","        stitch_image(ndarray) : stitched final image\n","    \"\"\"\n","\n","    print(\"stiching image ...\")\n","    \n","    # Convert to double and normalize. Avoid noise.\n","    left = cv2.normalize(left.astype('float'), None, \n","                            0.0, 1.0, cv2.NORM_MINMAX)   \n","    # Convert to double and normalize.\n","    right = cv2.normalize(right.astype('float'), None, \n","                            0.0, 1.0, cv2.NORM_MINMAX)   \n","    \n","    # left image\n","    height_l, width_l, channel_l = left.shape\n","    corners = [[0, 0, 1], [width_l, 0, 1], [width_l, height_l, 1], [0, height_l, 1]]\n","    corners_new = [np.dot(H, corner) for corner in corners]\n","    corners_new = np.array(corners_new).T \n","    x_news = corners_new[0] / corners_new[2]\n","    y_news = corners_new[1] / corners_new[2]\n","    y_min = min(y_news)\n","    x_min = min(x_news)\n","\n","    translation_mat = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n","    H = np.dot(translation_mat, H)\n","    \n","    # Get height, width\n","    height_new = int(round(abs(y_min) + height_l))\n","    width_new = int(round(abs(x_min) + width_l))\n","    size = (width_new, height_new)\n","\n","    # right image\n","    warped_l = cv2.warpPerspective(src=left, M=H, dsize=size)\n","\n","    height_r, width_r, channel_r = right.shape\n","    \n","    height_new = int(round(abs(y_min) + height_r))\n","    width_new = int(round(abs(x_min) + width_r))\n","    size = (width_new, height_new)\n","    \n","\n","    warped_r = cv2.warpPerspective(src=right, M=translation_mat, dsize=size)\n","     \n","    black = np.zeros(3)  # Black pixel.\n","    \n","    # Stitching procedure, store results in warped_l.\n","    for i in tqdm(range(warped_r.shape[0])):\n","        for j in range(warped_r.shape[1]):\n","            pixel_l = warped_l[i, j, :]\n","            pixel_r = warped_r[i, j, :]\n","            \n","            if not np.array_equal(pixel_l, black) and np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = pixel_l\n","            elif np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = pixel_r\n","            elif not np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n","                warped_l[i, j, :] = (pixel_l + pixel_r) / 2\n","            else:\n","                pass\n","                  \n","    stitch_image = warped_l[:warped_r.shape[0], :warped_r.shape[1], :]\n","    return stitch_image\n","    \n","\n","def plot_sift(gray_img, rgb_img, keypnt):\n","    \"\"\"\n","    plot_sift(gray_img, rgb_img, keypnt) -\u003e sift_on_img\\n\n","    Draw keypoints from gray image to rgb image.\n","\n","    input\n","        gray_img(ndarray) : image converted to gray\n","        rgb_img(ndarray) : original rgb image\n","        keypnt(ndarray) : keypoints of image\n","    output\n","        sift_on_img(ndarray) : rgb image with keypoints\n","    \"\"\"\n","    tmp_img = rgb_img.copy()\n","    sift_on_img = cv2.drawKeypoints(gray_img, keypnt, tmp_img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","    return sift_on_img\n","\n","    \n","def plot_matches(matches, total_img):\n","    \"\"\"\n","    plot_matches(matches, total_img)\\n\n","    Shows matched keypoints connected by a line.\n","    \n","    input\n","        matches(ndarray) : array of matched keypoints\n","        total_img(ndarray) : Image combining two images\n","    \"\"\"\n","    match_img = total_img.copy()\n","    offset = total_img.shape[1]/2\n","    fig, ax = plt.subplots()\n","    ax.set_aspect('equal')\n","    ax.imshow(np.array(match_img).astype('uint8')) #　RGB is integer type\n","    \n","    ax.plot(matches[:, 0], matches[:, 1], 'xr')\n","    ax.plot(matches[:, 2] + offset, matches[:, 3], 'xr')\n","     \n","    ax.plot([matches[:, 0], matches[:, 2] + offset], [matches[:, 1], matches[:, 3]],\n","            'r', linewidth=0.5)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iq9FG-I6PFd-"},"source":["## **[Homework III-2]** RANSAC Algorithm\n","* This is main part in HW3\n","* Carefully read and understand the following RANSAC code, \n","* Then please **complete the incomplete code:** `varable_name = [ ]`"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":489,"status":"ok","timestamp":1682154921444,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"H5W3_DY3PF3R"},"outputs":[],"source":["##############################################\n","#                   RANSAC                   #\n","##############################################\n","def ransac(matches, k_samples, threshold, iters):\n","    \"\"\"\n","    ransac(matches, k_samples, threshold, iters) -\u003e best inliers, best_H\\n\n","    Return the transformation matrix with the maximum consensus and its inliers with the RANSAC algorithm.\n","    \n","    input\n","        matches(ndarray) : array of matched keypoints\n","        k_samples(int) : the number of random points\n","        threshold(float) : The error threshold of the matched keypoints and matrix transformation results\n","        iters(int) : number of iterations\n","    output\n","        best_inliers(ndarray) : best inliers computed with ransac\n","        best_H(ndarray) : best transformation matrix computed with ransac\n","    \"\"\"\n","\n","    num_best_inliers = 0\n","    \n","    for i in range(iters):\n","        rand_pnt = randomPoint(matches, k_samples)\n","        H = homography(rand_pnt)\n","        \n","        # Avoid dividing by zero \n","        if np.linalg.matrix_rank(H) \u003c 3:\n","            continue\n","            \n","        errs = getError(matches, H)\n","        idx = np.where(errs \u003c threshold)[0]\n","        inliers = matches[idx]\n","\n","        num_inliers = len(inliers)\n","\n","        if num_inliers \u003e num_best_inliers:\n","            # Find the best inliers and transformation matrix, H            \n","            best_inliers = inliers\n","            num_best_inliers = num_inliers\n","            best_H = H\n","            \n","    print(\"inliers/matches: {}/{}\".format(num_best_inliers, len(matches)))\n","    return best_inliers, best_H\n","\n","def randomPoint(matches, k_samples):\n","    \"\"\"\n","    randomPoint(matches, k_samples) -\u003e rand_pnt\\n\n","    Return the randomly sampled keypoints.\n","\n","    input\n","        matches(ndarray) : array of matched keypoints\n","        k_samples(int) : the number of random points\n","    output\n","        rand_pnt(ndarray) : random sample points of keypoints array\n","    \"\"\"\n","\n","    rand_pnt = random.sample(matches.tolist(), k_samples)\n","        \n","    return np.array(rand_pnt) \n","\n","def getError(matches, H):\n","    \"\"\"\n","    getError(matches, H) -\u003e errors\\n\n","    Calculate errors by comparing matching points with the matrix transformation points.\n","    \n","    input\n","        matches(ndarray) : array of matched keypoints\n","        H(ndarray) : 3x3 projective transformation matrix\n","    output\n","        errors(ndarray) : difference between keypoints and matrix transformation values\n","    \"\"\"\n","\n","    num_points = len(matches)\n","    all_p1 = np.concatenate((matches[:, 0:2], np.ones((num_points, 1))), axis=1)\n","    all_p2 = matches[:, 2:4]\n","\n","    est_p2 = np.zeros((num_points, 2))\n","    for i in range(num_points):\n","        # Calculate the estimated p2\n","        # The estimated p2 = H*p1\n","        temp_est_p2 = H @ all_p1[i]\n","\n","        est_p2[i] = (temp_est_p2/temp_est_p2[2])[0:2] # set index 2 to 1 and slice the index 0, 1\n","        \n","    # Compute error\n","    errors = np.linalg.norm(all_p2 - est_p2, axis=1)\n","    \n","    return errors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360,"status":"ok","timestamp":1681375446149,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"CVqkJ2RtyKaY","outputId":"96687973-9e22-4e7c-d9f6-aa49735db4ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"NNKzsdjhBnTu"},"source":["## **[Homework III-3]** Run the Man Function and Get Panorama Image!\n","* Try to change various variables such as the number of samples, threshold, the number of iterations in RANSAC function\n","* Try to get your own panorama image with the pictures you took"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11CwoTSUBdJwrWJctvl7GVgGQVozlTvup"},"executionInfo":{"elapsed":69771,"status":"ok","timestamp":1682154995501,"user":{"displayName":"민정우","userId":"07570981936860096818"},"user_tz":-540},"id":"HGcXIcCW6M3O","outputId":"56b44863-dc3a-429e-a179-2641e2f7f919"},"outputs":[],"source":["##############################################\n","#                Main Function               #\n","##############################################\n","\n","if __name__ == \"__main__\":\n","\n","    fname_left = 'img_left.jpg'\n","    fname_right = 'img_right.jpg'\n","\n","    img_left  = cv2.resize(cv2.imread(dir + fname_left), dsize=(720, 720))\n","    img_right = cv2.resize(cv2.imread(dir + fname_right), dsize=(720, 720))\n","\n","    gray_left = cv2.cvtColor(img_left, cv2.COLOR_RGB2GRAY)\n","    gray_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n","    \n","    kp_left, des_left = SIFT(gray_left)\n","    kp_right, des_right = SIFT(gray_right)\n","\n","    print(len(kp_left))   # The number of keypoints on the left image\n","    print(des_left.shape) # The 128-dimensional descriptor for each keypoint\n","\n","    kp_left_img = plot_sift(gray_left, img_left, kp_left)\n","    kp_right_img = plot_sift(gray_right, img_right, kp_right)\n","    #total_kp = np.concatenate((kp_left_img, kp_right_img), axis=1)\n","    #plt.imshow(total_kp)\n","\n","    matches = siftMatch(kp_left, des_left, img_left, kp_right, des_right, img_right, 0.5)   # The coordinates of the matched correspondences p1=[x, y] and p2=[x', y']\n","    temp_img = np.concatenate((img_left, img_right), axis=1)\n","    plot_matches(matches, temp_img) # Good mathces\n","\n","    print(matches.shape)  # The number of well-matched keypoints (i.e., correspondences)\n","    \n","    \n","    inliers, H = ransac(matches, (int)(matches.shape[0] * 0.5), 0.5, 1000)\n","    plot_matches(inliers, temp_img) # show inliers matches\n","    \n","    panoramic_img = stitchImg(img_left, img_right, H)*255\n","    cv2_imshow(panoramic_img)\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"006f9174d4424718855ebcf3a9303ea7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4bb2c1637f46deaaf19289bd3fa2b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d74dcba2f04e249cd428a7d7c585d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f492e6baf2e45e29b79212b2515ea70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_006f9174d4424718855ebcf3a9303ea7","placeholder":"​","style":"IPY_MODEL_de3c6f736e914d0eb7ec4bb546adc20e","value":"100%"}},"6e02964aff6a46058f5a362d80ced99d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f492e6baf2e45e29b79212b2515ea70","IPY_MODEL_e4872fd301ef4040ae8fc631caa1cd26","IPY_MODEL_9e7d4528af9a40c8b35fa6663a609405"],"layout":"IPY_MODEL_91857f0b781744199eba9d40842acf6c"}},"76fe905569c54e6294a7770005772fca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91857f0b781744199eba9d40842acf6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e7d4528af9a40c8b35fa6663a609405":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5280396649c467e92dbfba6c2b3bd62","placeholder":"​","style":"IPY_MODEL_42d74dcba2f04e249cd428a7d7c585d1","value":" 986/986 [00:34\u0026lt;00:00, 30.74it/s]"}},"de3c6f736e914d0eb7ec4bb546adc20e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4872fd301ef4040ae8fc631caa1cd26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e4bb2c1637f46deaaf19289bd3fa2b5","max":986,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76fe905569c54e6294a7770005772fca","value":986}},"f5280396649c467e92dbfba6c2b3bd62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}